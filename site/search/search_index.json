{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"High Performance Computing | Research | Innovation        Empowering Research Through Computing Excellence      <p>The University of Alberta\u2019s SDRE is a secure, high-performance computing platform designed for research involving sensitive data.</p> <p>Built to meet stringent security and compliance standards, SDRE enables researchers to analyze large, complex datasets efficiently while supporting collaboration and safeguarding data integrity.</p> <p>This portal offers resources and guidance to help you get started and succeed in your work within the SDRE.</p> SDRE Basics and General SDRE overview Request an account for research project Connect to HPC Linux introduction Learn More High Performance Computing Schdule your job Available softwares Storage and file system Data transfer and share Learn More Learning Resources How to connect to VPN and VCS How to use SSH to SDRE Submit your job using Slurm Research computing bootcamp Learn More"},{"location":"pages/quicklink/","title":"Quicklink","text":"Systems and services <ul> <li>2025 infrastructure renewal</li> <li>General-purpose clusters</li> <ul> <li>B\u00e9luga, Cedar, Graham</li> </ul> <li>Services</li> <ul> <li>Available software</li> <li>Cloud computing service</li> </ul> </ul> How-to guides <ul> <li>Getting started</li> <ul> <li>Niagara Quick Start Guide</li> <li>Cloud Quick Start Guide</li> </ul> <li>Running jobs</li> <ul> <li>Installing software yourself</li> <li>Programming guide</li> </ul> </ul> Research Resources <ul> <li>Nextcloud storage</li> <li>Research Software</li> </ul> Support &amp; FAQs <ul> <li>Technical Support</li> <li>FAQs</li> </ul>"},{"location":"pages/storagefile/","title":"Storage and File Management","text":""},{"location":"pages/storagefile/#storage-types","title":"Storage Types","text":"<p>Unlike your personal computer, our systems will typically have several storage spaces or filesystems and you should ensure that you are using the right space for the right task. In this section we will discuss the principal filesystems available on most of our systems and the intended use of each one along with some of its characteristics.</p> <ul> <li> <p>HOME: While your home directory may seem like the logical place to store all your files and do all your work, in general this isn't the case; your home normally has a relatively small quota and doesn't have especially good performance for writing and reading large amounts of data. The most logical use of your home directory is typically source code, small parameter files and job submission scripts.</p> </li> <li> <p>PROJECT: The project space has a significantly larger quota and is well adapted to sharing data among members of a research group since it, unlike the home or scratch, is linked to a professor's account rather than an individual user. The data stored in the project space should be fairly static, that is to say the data are not likely to be changed many times in a month. Otherwise, frequently changing data, including just moving and renaming directories, in project can become a heavy burden on the tape-based backup system.</p> </li> <li> <p>SCRATCH: For intensive read/write operations on large files (&gt; 100 MB per file), scratch is the best choice. However, remember that important files must be copied off scratch since they are not backed up there, and older files are subject to purging. The scratch storage should therefore be used for temporary files: checkpoint files, output from jobs and other data that can easily be recreated. Do not regard SCRATCH as your normal storage! It is for transient files that you can afford to lose.</p> </li> <li> <p>SLURM_TMPDIR: While a job is running, the environment variable $SLURM_TMPDIR holds a unique path to a temporary folder on a fast, local filesystem on each compute node allocated to the job. When the job ends, the directory and its contents are deleted, so $SLURM_TMPDIR should be used for temporary files that are only needed for the duration of the job. Its advantage, compared to the other networked filesystem types above, is increased performance due to the filesystem being local to the compute node. It is especially well-suited for large collections of small files (for example, smaller than a few megabytes per file). Note that this filesystem is shared between all jobs running on the node, and that the available space depends on the compute node type. A more detailed discussion of using $SLURM_TMPDIR is available at this page.</p> </li> </ul>"},{"location":"pages/storagefile/#filesystem-quotas-and-policies","title":"Filesystem Quotas and Policies","text":"CedarGrahamB\u00e9luga and NarvalNiagara Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes"},{"location":"pages/basics/account_request/","title":"Account Request","text":"<p>Please fill out the below form in order to provide us with information about the computing work you would like to do (to the best of your knowledge).</p> <p>All accounts require an active RES number, CCID and a faculty sponsor (typically the PI that is leading the research project).</p> <p>Due to the nature of SDRE environment, we only accept projects meeting suitablity criteria. Please contact staff at SDRE for pre-application consulation.</p>"},{"location":"pages/basics/account_request/#current-faculty-of-university-of-alberta","title":"Current Faculty of University of Alberta","text":"<p>If you are a faculty at University of Alberta, you can request an account by completing the Account Request Form. A staff member from IT department will follow up with next steps.</p> <p>Account Request Form</p> <p>After filling out this form, CHTC Facilitation staff will follow up to evaluate your application and offer times for an initial consultation. </p>"},{"location":"pages/basics/account_request/#current-researcher-postdoc-graduate-and-undergraduate-student-of-university-of-alberta","title":"Current Researcher (postdoc, graduate and undergraduate student) of University of Alberta","text":"<p>Please make sure the project you are working on already has an account. The account request for researchers/students can only be submitted by the sponsor/project PI.</p>"},{"location":"pages/basics/account_request/#external-collaborator","title":"External Collaborator","text":"<p>If you are not a current member of University of Alberta, you can gain access to SDRE provided that you are sponsored by a faculty member of University of Alberta who has already set up a project/planned to request a project account in SDRE. To begin the account request process, have your Faculty Sponsor email SDRE (SDRE email) and provide:</p> <ul> <li>Your name,</li> <li>The reason you need (continued) access to SDRE resources,</li> <li>The amount of time they would like to sponsor your account,</li> <li>Your city/country of residence, and</li> <li>Your institution.</li> </ul> <p>SDRE staff will then follow up with next steps to create or extend your account.</p> <p>Your faculty sponsor can sponsor your account for up to one year at a time. If you need continued access past one year, your faculty sponsor must contact us and re-confirm that you should have continued access.</p> <p>Our policy is that SDRE accounts are deactivated and user data is erased after a user is no longer actively using their account (~1 year of inactivity). It is your responsibility to maintain your data and important files in a location that is not SDRE\u2019s file systems.</p>"},{"location":"pages/basics/connect_to_hpc/","title":"Connecting to SDRE","text":"<p>This page describes how to connect to the High Performance Computing cluster (HPC) and how to use the features available on the login nodes.</p>"},{"location":"pages/basics/connect_to_hpc/#connecting","title":"Connecting","text":"<p>To connect to the HPC, you must either be on-campus or be connected to the UofA VPN.</p>"},{"location":"pages/basics/connect_to_hpc/#users-who-own-an-unverified-device","title":"Users who own an unverified device","text":"<ol> <li>You must connect to UofA VPN or UofA guest VPN;</li> <li>Connect to your VCS;</li> <li>Use SSH to access SDRE. </li> </ol>"},{"location":"pages/basics/connect_to_hpc/#users-who-own-a-verifed-uofa-device","title":"Users who own a verifed UofA device","text":"<ol> <li>You must connect to SDRE VPN;</li> <li>Use SSH to access SDRE.</li> </ol>"},{"location":"pages/basics/connect_to_hpc/#using-ssh","title":"Using SSH","text":""},{"location":"pages/basics/connect_to_hpc/#overview","title":"Overview","text":"<p>Secure Shell (SSH) is a widely used standard to connect to remote machines securely. The SSH connection is encrypted, including the username and password. SSH is the standard way for you to connect in order to execute commands, submit jobs, check the progress of jobs, and in some cases, transfer files.</p> <p>Various implementations of the SSH standard exist for most major operating systems.</p> <ul> <li>On macOS and Linux, the most widely used client is OpenSSH, a command line application installed by default.</li> <li>For recent versions of Windows, SSH is available in the PowerShell terminal, in the cmd prompt, or through Windows Subsystem for Linux (WSL). There are also 3rd-party SSH clients that are popular, such as PuTTY, MobaXTerm, WinSCP, and Bitvise.</li> </ul> <p>To use any of these implementations of SSH successfully, you must:</p> <ul> <li>know the name of the machine to which you want to connect. This will be something like <code>sdre.ualberta.ca</code>.</li> <li>know your username, typically something like <code>ansmith</code>. The <code>username</code> is not your CCID, like <code>jsmith28</code>, nor your email address.</li> <li>know your password, or have an SSH key. Your password may not be the same one you use to log in to CCID. You may register and use an SSH key instead of a password; we highly recommend this since it provides better security.</li> <li>be registered for multifactor authentication and have your 2nd factor available.</li> </ul> <p>From a command-line client (e.g. /Applications/Utilities/Terminal.app for macOS, cmd or PowerShell for Windows), use the <code>ssh</code> command like this:</p> <pre><code>[name@server ~]$ ssh username@machine_name\n</code></pre> <p>For graphical clients such as MobaXterm or PuTTY, see:</p> <ul> <li>Connecting with MobaXTerm</li> <li>Connecting with PuTTY</li> </ul> <p>The first time that you connect to a machine you'll be asked to store a copy of its host key, a unique identifier that allows the SSH client to verify, when connecting next time, that this is the same machine.</p> <p>For more on generating key pairs, see:</p> <ul> <li>SSH Keys</li> <li>Generating SSH keys in Windows</li> <li>Using SSH keys in Linux</li> </ul> <p>For how to use SSH to allow communication between compute nodes and the internet, see:</p> <ul> <li>SSH tunnelling</li> </ul> <p>For how to use an SSH configuration file to simplify the login procedure, see:</p> <ul> <li>SSH configuration file</li> </ul>"},{"location":"pages/basics/connect_to_hpc/#connection-errors","title":"Connection Errors","text":"<p>While connecting to one of our clusters, you might get an error message such as:</p> <ul> <li>no matching cipher found</li> <li>no matching MAC found</li> <li>unable to negotiate a key exchange method</li> <li>couldn't agree a key exchange algorithm</li> <li>remote host identification has changed.</li> </ul> <p>The last of these error messages can point to a man-in-the-middle attack, or to an upgrade of security of the cluster you are trying to connect to. If you get this, verify that the host key fingerprint mentioned in the message matches one of the host key fingerprints published at SSH host keys. If it does, it is safe to continue connecting. If the host key fingerprint does not appear on our published list, terminate the connection and contact support.</p>"},{"location":"pages/basics/linux_intro/","title":"Linux Introduction","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Linux_introduction */</p> <p>This article is aimed at Windows and Mac users who do not have or have very little experience in UNIX environments. It should give you the necessary basics to access the compute servers and being quickly able to use them.</p> <p>Connections to the servers use the SSH protocol, in text mode. You do not use a graphical interface (GUI) but a console. Note that Windows executables do not run on our servers without using an emulator.</p>"},{"location":"pages/basics/linux_intro/#obtaining-help","title":"Obtaining help","text":"<p>Generally, UNIX commands are documented in the reference manuals that are available on the servers. To access those from a terminal:</p> <pre><code>[name@server ~]$ man command\n</code></pre> <p><code>man</code> uses <code>less</code> (see the section Viewing and editing files), and you must press <code>q</code> to exit this program.</p> <p>By convention, the executables themselves contain some help on how to use them. Generally, you invoke this help using the command line argument <code>-h</code> or <code>--help</code>, or in certain cases, <code>-help</code>. For example,</p> <pre><code>[name@server ~]$ ls --help\n</code></pre>"},{"location":"pages/basics/linux_intro/#orienting-yourself-on-a-system","title":"Orienting yourself on a system","text":"<p>Following your connection, you are directed to your <code>$HOME</code> directory (the UNIX word for folder) for your user account. When your account is created, your <code>$HOME</code> only contains a few hidden configuration files that start with a period (.), and nothing else.</p> <p>On a Linux system, you are strongly discouraged to create files or directories that contain names with spaces or special characters, including accents.</p>"},{"location":"pages/basics/linux_intro/#listing-directory-contents","title":"Listing directory contents","text":"<p>To list all files in a directory in a terminal, use the <code>ls</code> (list) command:</p> <pre><code>[name@server ~]$ ls\n</code></pre> <p>To include hidden files:</p> <pre><code>[name@server ~]$ ls -a\n</code></pre> <p>To sort results by date (from newest to oldest) instead of alphabetically:</p> <pre><code>[name@server ~]$ ls -t\n</code></pre> <p>And, to obtain detailed information on all files (permissions, owner, group, size and last modification date):</p> <pre><code>[name@server ~]$ ls -l\n</code></pre> <p>The option <code>-h</code> gives the file sizes in human readable format.</p> <p>You can combine options, for example:</p> <pre><code>[name@server ~]$ ls -alth\n</code></pre>"},{"location":"pages/basics/linux_intro/#navigating-the-filesystem","title":"Navigating the filesystem","text":"<p>To move about in the filesystem, use the <code>cd</code> command (change directory).</p> <p>So, to change to my_directory, type:</p> <pre><code>[name@server ~]$ cd my_directory\n</code></pre> <p>To change to the parent folder, type:</p> <pre><code>[name@server ~]$ cd ..\n</code></pre> <p>And, to move back to your home directory (<code>$HOME</code>):</p> <pre><code>[name@server ~]$ cd\n</code></pre>"},{"location":"pages/basics/linux_intro/#creating-and-removing-directories","title":"Creating and removing directories","text":"<p>To create (make) a directory, use the <code>mkdir</code> command:</p> <pre><code>[name@server ~]$ mkdir my_directory\n</code></pre> <p>To remove a directory, use the <code>rmdir</code> command:</p> <pre><code>[name@server ~]$ rmdir my_directory\n</code></pre> <p>Deleting a directory like this only works if it is empty.</p>"},{"location":"pages/basics/linux_intro/#deleting-files","title":"Deleting files","text":"<p>You can remove files using the <code>rm</code> command:</p> <pre><code>[name@server ~]$ rm my_file\n</code></pre> <p>You can also recursively remove a directory:</p> <pre><code>[name@server ~]$ rm -r my_directory\n</code></pre> <p>The (potentially dangerous!) <code>-f</code> option can be useful to bypass confirmation prompts and to continue the operation after an error.</p>"},{"location":"pages/basics/linux_intro/#copying-and-renaming-files-or-directories","title":"Copying and renaming files or directories","text":"<p>To copy a file use the <code>cp</code> command:</p> <pre><code>[name@server ~]$ cp source_file destination_file\n</code></pre> <p>To recursively copy a directory:</p> <pre><code>[name@server ~]$ cp -R source_directory destination_directory\n</code></pre> <p>To rename a file or a folder (directory), use the <code>mv</code> command (move):</p> <pre><code>[name@server ~]$ mv source_file destination_file\n</code></pre> <p>This command also applies to directories. You should then replace <code>source_file</code> with <code>source_directory</code> and <code>estination_file</code> with <code>destionation_directory</code>.</p>"},{"location":"pages/basics/linux_intro/#file-permissions","title":"File permissions","text":"<p>UNIX systems support 3 types of permissions : read (<code>r</code>), write (<code>w</code>) and execute (<code>x</code>). For files, a file should be readable to be read, writable to be modified, and executable to be run (if it's a binary executable or a script). For a directory, read permissions are necessary to list its contents, write permissions enable modification (adding or removing a file) and execute permissions enable changing to it.</p> <p>Permissions apply to 3 different classes of users, the owner (<code>u</code>), the group (<code>g</code>), and all others or the world (<code>o</code>). To know which permissions are associated to files and subdirectories of the current directory, use the following command:</p> <pre><code>[name@server ~]$ ls -la\n</code></pre> <p>The 10 characters at the beginning of each line show the permissions. The first character indicates the file type :</p> <ul> <li><code>-</code>: a normal file</li> <li><code>d</code>: a directory</li> <li><code>l</code>: a symbolic link</li> </ul> <p>Then, from left to right, this command shows read, write and execute permissions of the owner, the group and other users. Here are some examples :</p> <ul> <li><code>drwxrwxrwx</code>: a world-readable and world-writable directory</li> <li><code>drwxr-xr-x</code>: a directory that can be listed by everybody, but only the owner can add or remove files</li> <li><code>-rwxr-xr-x</code>: a world-readable and world-executable file that can only be changed by its owner</li> <li><code>-rw-r--r--</code>: a world-readable file that can only be changed by its owner.</li> <li><code>-rw-rw----</code>: a file that can be read and changed by its owner and by its group</li> <li><code>-rw-------</code>: a file that can only be read and changed by its owner</li> <li><code>drwx--x--x</code>: a directory that can only be listed or modified by its owner, but all others can still pass it on their way to a deeper subdirectory</li> <li><code>drwx-wx-wx</code>: a directory that everybody can enter and modify but where only the owner can list its contents</li> </ul> <p>Important note: to be able to read or write in a directory, you need to have execute permissions (<code>x</code>) set in all parent directories, all the way up to the filesystem's root (<code>/</code>). So if your home directory has <code>drwx------</code> permissions and contains a subdirectory with <code>drwxr-xr-x</code> permissions, other users cannot read the contents of this subdirectory because they do not have access (by the executable bit) to its parent directory.</p> <p>After listing the permissions, <code>ls -la</code> command gives a number, followed by the file owner's name, the file group's name, its size, last modification date, and name.</p> <p>The <code>chmod</code> command allows you to change file permissions. The simple way to use it is to specify which permissions you wish to add or remove to which type of user. To do this, you specify the list of users (<code>u</code> for the owner, <code>g</code> for the group, <code>o</code> for other users, <code>a</code> for all three), followed by a <code>+</code> to add permissions or <code>-</code> to remove permissions, which is then followed by a list of permissions to modify (<code>r</code> for read, <code>w</code> for write, <code>x</code> for execute). Non-specified permissions are not affected. Here are a few examples:</p> <ul> <li>Prevent group members and all others to read or modify the file <code>secret.txt</code>:</li> </ul> <pre><code>[name@server ~]$ chmod go-rwx secret.txt\n</code></pre> <ul> <li>Allow everybody to read the file <code>public.txt</code>:</li> </ul> <pre><code>[name@server ~]$ chmod a+r public.txt\n</code></pre> <ul> <li>Make the file <code>script.sh</code> executable:</li> </ul> <pre><code>[name@server ~]$ chmod a+x script.sh\n</code></pre> <ul> <li>Allow group members to read and write in the directory <code>shared</code>:</li> </ul> <pre><code>[name@server ~]$ chmod g+rwx shared\n</code></pre> <ul> <li>Prevent other users from reading or modifying your home directory:</li> </ul> <pre><code>[name@server ~]$ chmod go-rw ~\n</code></pre>"},{"location":"pages/basics/linux_intro/#viewing-and-editing-files","title":"Viewing and editing files","text":""},{"location":"pages/basics/linux_intro/#viewing-a-file","title":"Viewing a file","text":"<p>To view a file read-only, use the <code>less</code> command:</p> <pre><code>[name@server ~]$ less file_to_view\n</code></pre> <p>You can then use the arrow keys or the mouse wheel to navigate the document. You can search for something in the document by typing <code>/what_to_search_for</code>. You can quit <code>less</code> by pressing the <code>q</code> key.</p>"},{"location":"pages/basics/linux_intro/#comparing-two-files","title":"Comparing two files","text":"<p>The <code>diff</code> command allows you to compare two files:</p> <pre><code>[name@server ~]$ diff file1 file2\n</code></pre> <p>The <code>-y</code> option shows both files side by side.</p>"},{"location":"pages/basics/linux_intro/#searching-within-a-file","title":"Searching within a file","text":"<p>The <code>grep</code> command allows you to look for a given expression in one file:</p> <pre><code>[name@server ~]$ grep 'tata' file1\n</code></pre> <p>... or in multiple files:</p> <pre><code>[name@server ~]$ grep 'tata' fil*\n</code></pre> <p>Note that, in Linux, the <code>*</code> wildcard matches zero or more characters. The <code>?</code> wildcard matches exactly one character.</p> <p>The text to be searched for can also be variable. For example, to look for the text number 10 or number 11, etc. with any number between 10 and 29, the following command can be used:</p> <pre><code>[name@server ~]$ grep 'number [1-2][0-9]' file\n</code></pre> <p>A regular expression must be used for the search text. To learn more, see this guide to regular expressions.</p>"},{"location":"pages/basics/overview/","title":"Overview","text":""},{"location":"pages/basics/overview/#who-we-are","title":"Who we are","text":"<p>The Sensitive Data Research Environment (SDRE) is a secure, scalable, stable and robust high-performance platform developed to support research involving legislated, regulated, or contractually sensitive data at the University of Alberta.</p> <p>Aligned with national research security guidelines and the university\u2019s strategic goals, SDRE offers access to advanced computing resources\u2014CPUs, GPUs, high-memory nodes, and fast storage\u2014along with expert support to help researchers accelerate sensitive-data projects and meet compliance requirements.</p>"},{"location":"pages/basics/overview/#getting-started","title":"Getting Started","text":"<p>Follow the steps below to access RCC services:</p> <ol> <li> <p>Activate and manage your SDRE user account through  portal/freshservice/.etc. Note: Most SDRE services require an SDRE user account, which is separate from your CCID</p> </li> <li> <ul> <li> <p>If you have verified UofA device, connect to the UofA SDRE VPN service</p> </li> <li> <p>If you are from UofA withoutd verified UofA device, connect to the UofA VPN service</p> </li> <li> <p>If you are the researcher outside of University of Alberta, connect to the UofA guest VPN service</p> </li> </ul> </li> <li> <p>Start using our resources:</p> <ul> <li> <p>For VCS access to the HPC, check out Connecting to HPC</p> </li> <li> <p>Read our documentation for connecting and using the HPC</p> </li> <li> <p>Learn how to submit HPC jobs</p> </li> <li> <p>For information about HPC storage, refer to the Parallel Storage webpage</p> </li> </ul> </li> </ol>"},{"location":"pages/basics/overview/#getting-help","title":"Getting Help","text":"<p>If you are stuck, we have a few resources that may help:</p> <ul> <li> <p>Search through our documentation in the upper-right corner of this site</p> <ul> <li>Hint: type the <code>/</code> (forward slash) key to activate the search box</li> </ul> </li> <li> <p>Read through our learning resources to see a round-up of educational materials</p> </li> <li> <p>Submit a support request by sending an email to email address or submitting a ticket through Freshservice.</p> </li> </ul>"},{"location":"pages/basics/sdre_user_policy/","title":"SDRE User Policy","text":"<p>This page is under construction...</p>"},{"location":"pages/basics/system_specifications/","title":"HPC Hardware and Configuration","text":"<p>This page is under construction...</p> <p>The HPC Cluster consists of one login node and many compute (aka execute) nodes. All users log in at a login node, and all user files on the shared file sytem are accessible on all nodes. Additionally, all nodes are tightly networked (200 Gbit/s Infiniband) so they can work together as a single \"supercomputer\", depending on the number of CPUs you specify.</p>"},{"location":"pages/basics/system_specifications/#operating-system-and-software","title":"Operating System and Software","text":"<p>All nodes in the HPC Cluster are running OS.</p> <p>The SLURM scheduler version is SLURM version.</p> <p>To see more details of other software on the cluster, see the Available Software page.</p>"},{"location":"pages/basics/system_specifications/#login-node","title":"Login Node","text":"<p>The login node for the cluster is: address. </p> <p>For more details on logging in, see the \u201cConnecting to SDRE\u201d guide linked above.</p>"},{"location":"pages/basics/system_specifications/#compute-nodes-and-partitions","title":"Compute Nodes and Partitions","text":"<p>Only compute nodes will be used for performing your computational work. The execute nodes are organized into several \"partitions\", including the name partitions which are available to all HPC users as well as research group specific partitions that consist of researcher-owned hardware and which all HPC users can access on a backfill capacity via the pre partition (more details below).</p> <p>Details</p>"},{"location":"pages/basics/system_specifications/#sdre-specific-policies","title":"SDRE specific policies","text":"<p>By policy, SDRE's compute nodes cannot access the internet. If you need an exception to this rule, contact technical support explaining what you need and why.</p> <p>Crontab is not offered on SDRE.</p> <p>Each job on SDRE should have a duration of at least one hour (five minutes for test jobs) and you cannot have more than 1000 jobs, running or queued, at any given moment. The maximum duration for a job on SDRE is 7 days (168 hours).</p>"},{"location":"pages/basics/system_specifications/#high-performance-interconnect","title":"High-performance interconnect","text":"<p>The InfiniBand Mellanox HDR network links together all of the nodes of the cluster. Each hub of 40 HDR ports (200 Gb/s) can connect up to 66 nodes with HDR100 (100 Gb/s) with 33 HDR links divided in two (2) by special cables. The seven (7) remaining HDR links allow the hub to be connected to a rack containing the seven (7) central HDR InfiniBand hubs. The islands of nodes are therefore connected by a maximum blocking factor of 33:7 (4.7:1). In contrast, the storage servers are connected by a much lower blocking factor in order to maximize the performance.</p> <p>In practice the Narval racks contain islands of 48 or 56 regular CPU nodes. It is therefore possible to run parallel jobs using up to 3584 cores with a non-blocking network. For larger jobs or ones which are distributed in a fragmented manner across the network, the blocking factor is 4.7:1. The inter-connect remains a high-performance one nonetheless.</p>"},{"location":"pages/basics/system_specifications/#node-characteristics","title":"Node characteristics","text":"<p> Table </p>"},{"location":"pages/high_perf_computing/job_arrays/","title":"Job arrays","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Job_arrays */</p> <p>If your work consists of a large number of tasks which differ only in some parameter, you can conveniently submit many tasks at once using a job array, also known as a task array or an array job. The individual tasks in the array are distinguished by an environment variable, <code>$SLURM_ARRAY_TASK_ID</code>, which Slurm sets to a different value for each task. You set the range of values with the <code>--array</code> parameter.</p> <p>See Job Array Support for more details.</p>"},{"location":"pages/high_perf_computing/job_arrays/#examples-of-the-array-parameter","title":"Examples of the <code>--array</code> parameter","text":"<pre><code>sbatch --array=0-7       # $SLURM_ARRAY_TASK_ID takes values from 0 to 7 inclusive\nsbatch --array=1,3,5,7   # $SLURM_ARRAY_TASK_ID takes the listed values\nsbatch --array=1-7:2     # Step size of 2, same as the previous example\nsbatch --array=1-100%10  # Allows no more than 10 of the jobs to run simultaneously\n</code></pre>"},{"location":"pages/high_perf_computing/job_arrays/#a-simple-example","title":"A simple example","text":"<p>\ud83d\udce5click to download</p> simple_array.sh<pre><code>#!/bin/bash\n#SBATCH --array=1-10\n#SBATCH --time=3:00:00\nprogram_x &lt;input.$SLURM_ARRAY_TASK_ID\nprogram_y $SLURM_ARRAY_TASK_ID some_arg another_arg\n</code></pre> <p>This job will be scheduled as ten independent tasks. Each task has a separate time limit of 3 hours, and each may start at a different time on a different host.</p> <p>The script references <code>$SLURM_ARRAY_TASK_ID</code> to select an input file (named program_x in our example), or to set a command-line argument for the application (as in program_y).</p> <p>Using a job array instead of a large number of separate serial jobs has advantages for you and other users. A waiting job array only produces one line of output in squeue, making it easier for you to read its output. The scheduler does not have to analyze job requirements for each task in the array separately, so it can run more efficiently too.</p> <p>Note that, other than the initial job-submission step with <code>sbatch</code>, the load on the scheduler is the same for an array job as for the equivalent number of non-array jobs. The cost of dispatching each array task is the same as dispatching a non-array job. You should not use a job array to submit tasks with very short run times, e.g. much less than an hour. Tasks with run times of only a few minutes should be grouped into longer jobs using META, GLOST, GNU Parallel, or a shell loop inside a job.</p>"},{"location":"pages/high_perf_computing/job_arrays/#example-multiple-directories","title":"Example: Multiple directories","text":"<p>Suppose you have multiple directories, each with the same structure, and you want to run the same script in each directory. If the directories can be named with sequential numbers then the example above can be easily adapted. If the names are not so systematic, then create a file with the names of the directories, like so:</p> <pre><code>$ cat case_list\npacific2016\npacific2017\natlantic2016\natlantic2017\n</code></pre> <p>There are several ways to select a given line from a file; this example uses <code>sed</code> to do so:</p> <p>\ud83d\udce5click to download</p> directories_array.sh<pre><code>#!/bin/bash\n#SBATCH --time=3:00:00\n#SBATCH --array=1-4\n\necho \"Starting task $SLURM_ARRAY_TASK_ID\"\nDIR=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" case_list)\ncd $DIR\n\n# Place the code to execute here\npwd\nls\n</code></pre> <p>Cautions:</p> <ul> <li>Take care that the number of tasks you request matches the number of entries in the file.</li> <li>The file <code>case_list</code> should not be changed until all the tasks in the array have run, since it will be read each time a new task starts.</li> </ul>"},{"location":"pages/high_perf_computing/job_arrays/#example-multiple-parameters","title":"Example: Multiple parameters","text":"<p>Suppose you have a Python script doing certain calculations with some parameters defined in a Python list or a NumPy array such as</p> <p>\ud83d\udce5click to download</p> my_script.py<pre><code>import time\nimport numpy as np\n\ndef calculation(x, beta):\n    time.sleep(2) #simulate a long run\n    return beta * np.linalg.norm(x**2)\n\nif __name__ == \"__main__\":\n    x = np.random.rand(100)\n    betas = np.linspace(10,36.5,100) #subdivise the interval [10,36.5] with 100 values\n    for i in range(len(betas)): #iterate through the beta parameter\n        res = calculation(x,betas[i])\n        print(res) #show the results on screen\n\n# Run with: python my_script.py\n</code></pre> <p>The above task can be processed in a job array so that each value of the beta parameter can be treated in parallel. The idea is to pass the $SLURM_ARRAY_TASK_ID to the Python script and get the beta parameter based on its value. The Python script becomes</p> <p>\ud83d\udce5click to download</p> my_script_parallel.py<pre><code>import time\nimport numpy as np\nimport sys\n\ndef calculation(x, beta):\n    time.sleep(2) #simulate a long run\n    return beta * np.linalg.norm(x**2)\n\nif __name__ == \"__main__\":\n    x = np.random.rand(100)\n    betas = np.linspace(10,36.5,100) #subdivise the interval [10,36.5] with 100 values\n\n    i = int(sys.argv[1]) #get the value of the $SLURM_ARRAY_TASK_ID\n    res = calculation(x,betas[i])\n    print(res) #show the results on screen\n\n# Run with: python my_script_parallel.py $SLURM_ARRAY_TASK_ID\n</code></pre> <p>The job submission script is (note the array parameters goes from 0 to 99 like the indexes of the NumPy array)</p> <p>\ud83d\udce5click to download</p> data_parallel_python.sh<pre><code>#!/bin/bash\n#SBATCH --array=0-99\n#SBATCH --time=1:00:00\nmodule load scipy-stack\npython my_script_parallel.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"pages/high_perf_computing/resubmitting_jobs/","title":"Resubmitting jobs","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Running_jobs#Resubmitting_jobs_for_long-running_computations */</p> <p>When a computation is going to require a long time to complete, so long that it cannot be done within the time limits on the system, the application you are running must support checkpointing. The application should be able to save its state to a file, called a checkpoint file, and then it should be able to restart and continue the computation from that saved state.</p> <p>For many users restarting a calculation will be rare and may be done manually, but some workflows require frequent restarts. In this case some kind of automation technique may be employed.</p> <p>Here are two recommended methods of automatic restarting:</p> <ul> <li>Using SLURM job arrays.</li> <li>Resubmitting from the end of the job script.</li> </ul> <p>Our Machine Learning tutorial covers resubmitting for long machine learning jobs.</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#restarting-using-job-arrays","title":"Restarting using job arrays","text":"<p>Using the <code>--array=1-100%10</code> syntax one can submit a collection of identical jobs with the condition that only one job of them will run at any given time. The script should be written to ensure that the last checkpoint is always used for the next job. The number of restarts is fixed by the <code>--array</code> argument.</p> <p>Consider, for example, a molecular dynamics simulations that has to be run for 1 000 000 steps, and such simulation does not fit into the time limit on the cluster. We can split the simulation into 10 smaller jobs of 100 000 steps, one after another.</p> <p>An example of using a job array to restart a simulation:</p> <p>``` sh title=\"job_array_restart.sh\" click to download</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#binbash","title":"!/bin/bash","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#-","title":"---------------------------------------------------------------------","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#slurm-script-for-a-multi-step-job-on-our-clusters","title":"SLURM script for a multi-step job on our clusters.","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_1","title":"---------------------------------------------------------------------","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#sbatch-accountdef-someuser","title":"SBATCH --account=def-someuser","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#sbatch-cpus-per-task1","title":"SBATCH --cpus-per-task=1","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#sbatch-time0-1000","title":"SBATCH --time=0-10:00","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#sbatch-mem100m","title":"SBATCH --mem=100M","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#sbatch-array1-101-run-a-10-job-array-one-job-at-a-time","title":"SBATCH --array=1-10%1   # Run a 10-job array, one job at a time.","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_2","title":"---------------------------------------------------------------------","text":"<p>echo \"Current working directory: <code>pwd</code>\" echo \"Starting run at: <code>date</code>\"</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_3","title":"---------------------------------------------------------------------","text":"<p>echo \"\" echo \"Job Array ID / Job ID: $SLURM_ARRAY_JOB_ID / $SLURM_JOB_ID\" echo \"This is job $SLURM_ARRAY_TASK_ID out of $SLURM_ARRAY_TASK_COUNT jobs.\" echo \"\"</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_4","title":"---------------------------------------------------------------------","text":""},{"location":"pages/high_perf_computing/resubmitting_jobs/#run-your-simulation-step-here","title":"Run your simulation step here...","text":"<p>if test -e state.cpt; then       # There is a checkpoint file, restart;      mdrun --restart state.cpt else      # There is no checkpoint file, start a new simulation.      mdrun fi</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_5","title":"---------------------------------------------------------------------","text":"<p>echo \"Job finished with exit code $? at: <code>date</code>\"</p>"},{"location":"pages/high_perf_computing/resubmitting_jobs/#-_6","title":"---------------------------------------------------------------------","text":"<pre><code>## **Resubmission from the job script**\n\n----\n\nIn this case one submits a job that runs the first chunk of the calculation and saves a checkpoint. Once the chunk is done but before the allocated run-time of the job has elapsed, the script checks if the end of the calculation has been reached. If the calculation is not yet finished, the script submits a copy of itself to continue working.\n\nAn example of a job script with resubmission:\n\n``` sh title=\"job_resubmission.sh\" [click to download](/files/high_perf_computing/resubmitting_jobs/job_resubmission.sh){:download=\"job_resubmission.sh\"}\n\n#!/bin/bash\n# ---------------------------------------------------------------------\n# SLURM script for job resubmission on our clusters. \n# ---------------------------------------------------------------------\n#SBATCH --job-name=job_chain\n#SBATCH --account=def-someuser\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0-10:00\n#SBATCH --mem=100M\n# ---------------------------------------------------------------------\necho \"Current working directory: `pwd`\"\necho \"Starting run at: `date`\"\n# ---------------------------------------------------------------------\n# Run your simulation step here...\n\nif test -e state.cpt; then \n     # There is a checkpoint file, restart;\n     mdrun --restart state.cpt\nelse\n     # There is no checkpoint file, start a new simulation.\n     mdrun\nfi\n\n# Resubmit if not all work has been done yet.\n# You must define the function work_should_continue().\nif work_should_continue; then\n     sbatch ${BASH_SOURCE[0]}\nfi\n\n# ---------------------------------------------------------------------\necho \"Job finished with exit code $? at: `date`\"\n# ---------------------------------------------------------------------\n</code></pre> <p>Please note: The test to determine whether to submit a follow-up job, abbreviated as <code>work_should_continue</code> in the above example, should be a positive test. There may be a temptation to test for a stopping condition (e.g. is some convergence criterion met?) and submit a new job if the condition is not detected. But if some error arises that you didn't foresee, the stopping condition might never be met and your chain of jobs may continue indefinitely, doing nothing useful.</p>"},{"location":"pages/high_perf_computing/running_jobs/","title":"Running jobs","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Running_jobs#Examples_of_job_scripts */</p> <p>This page is intended for the user who is already familiar with the concepts of job scheduling and job scripts, and who wants guidance on submitting jobs to our clusters. If you have not worked on a large shared computer cluster before, you should probably read What is a scheduler? first.</p> <p>On our clusters, the job scheduler is the Slurm Workload Manager. Comprehensive documentation for Slurm is maintained by SchedMD.</p>"},{"location":"pages/high_perf_computing/running_jobs/#use-sbatch-to-submit-jobs","title":"Use <code>sbatch</code> to submit jobs","text":"<p>The command to submit a job is <code>sbatch</code>:</p> <pre><code>$ sbatch simple_job.sh\nSubmitted batch job 123456\n</code></pre> <p>A minimal Slurm job script looks like this:</p> <p>\ud83d\udce5click to download</p> simple_job.sh<pre><code>#!/bin/bash\n#SBATCH --time=00:15:00\n#SBATCH --account=def-someuser\necho 'Hello, world!'\nsleep 30\n</code></pre> <p>On SDRE environment, this job reserves 1 core and 256MB of memory for 15 minutes. Directives (or options) in the job script are prefixed with <code>#SBATCH</code> and must precede all executable commands. All available directives are described on the sbatch page. Our policies require that you supply at least a time limit (--time) for each job. You may also need to supply an account name (--account). See Accounts and projects below.</p> <p>You can also specify directives as command-line arguments to <code>sbatch</code>. So for example,</p> <pre><code>$ sbatch --time=00:30:00 simple_job.sh \n</code></pre> <p>will submit the above job script with a time limit of 30 minutes. The acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\". Please note that the time limit will strongly affect how quickly the job is started.</p> <p>Please be cautious if you use a script to submit multiple Slurm jobs in a short time. Submitting thousands of jobs at a time can cause Slurm to become unresponsive to other users. Consider using an array job instead, or use <code>sleep</code> to space out calls to <code>sbatch</code> by one second or more.</p>"},{"location":"pages/high_perf_computing/running_jobs/#memory","title":"Memory","text":"<p>Memory may be requested with --mem-per-cpu (memory per core) or --mem (memory per node). On SDRE, a default memory amount of 256 MB per core will be allocated unless you make some other request. </p> <p>A common source of confusion comes from the fact that some memory on a node is not available to the job (reserved for the OS, etc.). The effect of this is that each node type has a maximum amount available to jobs; for instance, nominally \"128G\" nodes are typically configured to permit 125G of memory to user jobs. If you request more memory than a node-type provides, your job will be constrained to run on higher-memory nodes, which may be fewer in number.</p> <p>Adding to this confusion, Slurm interprets K, M, G, etc., as binary prefixes, so <code>--mem=125G</code> is equivalent to <code>--mem=128000M</code>. See the Available memory column in the Node characteristics table for the maximum memory you can request on each node.</p>"},{"location":"pages/high_perf_computing/running_jobs/#use-squeue-or-sq-to-list-jobs","title":"Use <code>squeue</code> or <code>sq</code> to list jobs","text":"<p>The general command for checking the status of Slurm jobs is <code>squeue</code>, but by default it supplies information about all jobs in the system, not just your own. You can use the shorter <code>sq</code> to list only your own jobs:</p> <pre><code>$ sq\n   JOBID     USER      ACCOUNT      NAME  ST   TIME_LEFT NODES CPUS    GRES MIN_MEM NODELIST (REASON)\n  123456   smithj   def-smithj  simple_j   R        0:03     1    1  (null)      4G cdr234  (None)\n  123457   smithj   def-smithj  bigger_j  PD  2-00:00:00     1   16  (null)     16G (Priority)\n</code></pre> <p>The ST column of the output shows the status of each job. The two most common states are PD for pending or R for running.</p> <p>If you want to know more about the output of <code>sq</code> or <code>squeue</code>, or learn how to change the output, see the online manual page for squeue. <code>sq</code> is a local customization.</p> <p>Do not run sq or squeue from a script or program at high frequency (e.g. every few seconds). Responding to <code>squeue</code> adds load to Slurm, and may interfere with its performance or correct operation. See Email notification below for a much better way to learn when your job starts or ends.</p>"},{"location":"pages/high_perf_computing/running_jobs/#where-does-the-output-go","title":"Where does the output go?","text":"<p>By default the output is placed in a file named \"slurm-\", suffixed with the job ID number and \".out\" (e.g. <code>slurm-123456.out</code>), in the directory from which the job was submitted. Having the job ID as part of the file name is convenient for troubleshooting.</p> <p>A different name or location can be specified if your workflow requires it by using the --output directive. Certain replacement symbols can be used in a filename specified this way, such as the job ID number, the job name, or the job array task ID. See the vendor documentation on sbatch for a complete list of replacement symbols and some examples of their use.</p> <p>Error output will normally appear in the same file as standard output, just as it would if you were typing commands interactively. If you want to send the standard error channel (stderr) to a separate file, use <code>--error</code>.</p>"},{"location":"pages/high_perf_computing/running_jobs/#accounts-and-projects","title":"Accounts and projects","text":"<p>Information about your job, like how long it waited, how long it ran, and how many cores it used, is recorded so we can monitor our quality of service and so we can report to our funders how their money is spent. Every job must have an associated account name corresponding to a resource allocation project.</p> <pre><code>#SBATCH --account=def-user-ab\n</code></pre> <p>If you try to submit a job with <code>sbatch</code> without supplying an account name, and one is needed, you will be shown a list of valid account names to choose from.</p>"},{"location":"pages/high_perf_computing/running_jobs/#examples-of-job-scripts","title":"Examples of job scripts","text":""},{"location":"pages/high_perf_computing/running_jobs/#serial-job","title":"Serial job","text":"<p>A serial job is a job which only requests a single core. It is the simplest type of job. The \"simple_job.sh\" which appears above in Use sbatch to submit jobs is an example.</p>"},{"location":"pages/high_perf_computing/running_jobs/#array-job","title":"Array job","text":"<p>Also known as a task array, an array job is a way to submit a whole set of jobs with one command. The individual jobs in the array are distinguished by an environment variable, <code>$SLURM_ARRAY_TASK_ID</code>, which is set to a different value for each instance of the job. The following example will create 10 tasks, with values of <code>$SLURM_ARRAY_TASK_ID</code> ranging from 1 to 10:</p> <p>\ud83d\udce5click to download</p> array_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --time=0-0:5\n#SBATCH --array=1-10\n./myapplication $SLURM_ARRAY_TASK_ID\n</code></pre> <p>For more examples, see Job arrays. See Job Array Support for detailed documentation.</p>"},{"location":"pages/high_perf_computing/running_jobs/#threaded-or-openmp-job","title":"Threaded or OpenMP job","text":"<p>This example script launches a single process with eight CPU cores. Bear in mind that for an application to use OpenMP it must be compiled with the appropriate flag, e.g. <code>gcc -fopenmp ...</code> or <code>icc -openmp ...</code></p> <p>\ud83d\udce5click to download</p> openmp_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --time=0-0:5\n#SBATCH --cpus-per-task=8\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./ompHello\n</code></pre>"},{"location":"pages/high_perf_computing/running_jobs/#mpi-job","title":"MPI job","text":"<p>This example script launches four MPI processes, each with 1024 MB of memory. The run time is limited to 5 minutes.</p> <p>\ud83d\udce5click to download {:download=\"mpi_job.sh\"}</p> mpi_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --ntasks=4               # number of MPI processes\n#SBATCH --mem-per-cpu=1024M      # memory; default unit is megabytes\n#SBATCH --time=0-00:05           # time (DD-HH:MM)\nsrun ./mpi_program               # mpirun or mpiexec also work\n</code></pre> <p>Large MPI jobs, specifically those which can efficiently use whole nodes, should use <code>--nodes</code> and <code>--ntasks-per-node</code> instead of <code>--ntasks</code>. Hybrid MPI/threaded jobs are also possible. For more on these and other options relating to distributed parallel jobs, see Advanced MPI scheduling.</p> <p>For more on writing and running parallel programs with OpenMP, see OpenMP.</p>"},{"location":"pages/high_perf_computing/running_jobs/#gpu-job","title":"GPU job","text":"<p>Please see Using GPUs with Slurm for a discussion and examples of how to schedule various job types on the available GPU resources.</p>"},{"location":"pages/high_perf_computing/running_jobs/#interactive-jobs","title":"Interactive jobs","text":"<p>Though batch submission is the most common and most efficient way to take advantage of our clusters, interactive jobs are also supported. These can be useful for things like:</p> <ul> <li>Data exploration at the command line</li> <li>Interactive console tools like R and iPython</li> <li>Significant software development, debugging, or compiling</li> </ul> <p>You can start an interactive session on a compute node with salloc. In the following example we request one task, which corresponds to one CPU cores and 3 GB of memory, for an hour:</p> <pre><code>$ salloc --time=1:0:0 --mem-per-cpu=3G --ntasks=1 --account=def-someuser\nsalloc: Granted job allocation 1234567\n$ ...             # do some work\n$ exit            # terminate the allocation\nsalloc: Relinquishing job allocation 1234567\n</code></pre> <p>It is also possible to run graphical programs interactively on a compute node by adding the --x11 flag to your <code>salloc</code> command. In order for this to work, you must first connect to the cluster with X11 forwarding enabled (see the SSH page for instructions on how to do that). Note that an interactive job with a duration of three hours or less will likely start very soon after submission as we have dedicated test nodes for jobs of this duration. Interactive jobs that request more than three hours run on the cluster's regular set of nodes and may wait for many hours or even days before starting, at an unpredictable (and possibly inconvenient) hour.</p>"},{"location":"pages/high_perf_computing/running_jobs/#monitoring-jobs","title":"Monitoring jobs","text":""},{"location":"pages/high_perf_computing/running_jobs/#current-jobs","title":"Current jobs","text":"<p>By default squeue will show all the jobs the scheduler is managing at the moment. It will run much faster if you ask only about your own jobs with</p> <pre><code>$ squeue -u $USER\nYou can also use the utility sq to do the same thing with less typing.\n</code></pre> <p>You can show only running jobs, or only pending jobs:</p> <pre><code>$ squeue -u &lt;username&gt; -t RUNNING\n$ squeue -u &lt;username&gt; -t PENDING\n</code></pre> <p>You can show detailed information for a specific job with scontrol:</p> <pre><code>$ scontrol show job &lt;jobid&gt;\n</code></pre> <p>Do not run <code>squeue</code> from a script or program at high frequency (e.g., every few seconds). Responding to <code>squeue</code> adds load to Slurm and may interfere with its performance or correct operation.</p> <p>Email notification</p> <p>You can ask to be notified by email of certain job conditions by supplying options to sbatch:</p> <pre><code>#SBATCH --mail-user=your.email@example.com\n#SBATCH --mail-type=ALL\n</code></pre> <p>Please do not turn on these options unless you are going to read the emails they generate! It may lead to email service providers (Google, Yahoo, etc) restricting the flow of mail from the domains because one user is generating a huge volume of unnecessary emails via these options.</p> <p>For a complete list of the options for <code>--mail-type</code> see SchedMD's documentation.</p> <p>Output buffering</p> <p>Output from a non-interactive Slurm job is normally buffered, which means that there is usually a delay between when data is written by the job and when you can see the output on a login node. Depending on the application, you are running and the load on the filesystem, this delay can range from less than a second to many minutes, or until the job completes.</p> <p>There are methods to reduce or eliminate the buffering, but we do not recommend using them because buffering is vital to preserving the overall performance of the filesystem. If you need to monitor the output from a job in real time, we recommend you run an interactive job as described above.</p>"},{"location":"pages/high_perf_computing/running_jobs/#completed-jobs","title":"Completed jobs","text":"<p>Get a short summary of the CPU and memory efficiency of a job with <code>seff</code>:</p> <pre><code>$ seff 12345678\nJob ID: 12345678\nCluster: cedar\nUser/Group: jsmith/jsmith\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 02:48:58\nCPU Efficiency: 99.72% of 02:49:26 core-walltime\nJob Wall-clock time: 02:49:26\nMemory Utilized: 213.85 MB\nMemory Efficiency: 0.17% of 125.00 GB\n</code></pre> <p>Find more detailed information about a completed job with sacct, and optionally, control what it prints using <code>--format</code>:</p> <pre><code>$ sacct -j &lt;jobid&gt;\n$ sacct -j &lt;jobid&gt; --format=JobID,JobName,MaxRSS,Elapsed\n</code></pre> <p>The output from <code>sacct</code> typically includes records labelled .bat+ and .ext+, and possibly .0, .1, .2, .... The batch step (<code>.bat+</code>) is your submission script - for many jobs that's where the main part of the work is done and where the resources are consumed. If you use <code>srun</code> in your submission script, that would create a <code>.0</code> step that would consume most of the resources. The extern (<code>.ext+</code>) step is basically prologue and epilogue and normally doesn't consume any significant resources.</p> <p>If a node fails while running a job, the job may be restarted. <code>sacct</code> will normally show you only the record for the last (presumably successful) run. If you wish to see all records related to a given job, add the <code>--duplicates</code> option.</p> <p>Use the MaxRSS accounting field to determine how much memory a job needed. The value returned will be the largest resident set size for any of the tasks. If you want to know which task and node this occurred on, print the MaxRSSTask and MaxRSSNode fields also.</p> <p>The sstat command works on a running job much the same way that sacct works on a completed job.</p>"},{"location":"pages/high_perf_computing/running_jobs/#attching-to-a-running-job","title":"Attching to a running job","text":"<p>It is possible to connect to the node running a job and execute new processes there. You might want to do this for troubleshooting or to monitor the progress of a job.</p> <p>Suppose you want to run the utility <code>nvidia-smi</code> to monitor GPU usage on a node where you have a job running. The following command runs <code>watch</code> on the node assigned to the given job, which in turn runs <code>nvidia-smi</code> every 30 seconds, displaying the output on your terminal.</p> <pre><code>$ srun --jobid 123456 --pty watch -n 30 nvidia-smi\n</code></pre> <p>It is possible to launch multiple monitoring commands using <code>tmux</code>. The following command launches <code>htop</code> and <code>nvidia-smi</code> in separate panes to monitor the activity on a node assigned to the given job.</p> <pre><code>$ srun --jobid 123456 --pty tmux new-session -d 'htop -u $USER' \\; split-window -h 'watch nvidia-smi' \\; attach\n</code></pre> <p>Processes launched with <code>srun</code> share the resources with the job specified. You should therefore be careful not to launch processes that would use a significant portion of the resources allocated for the job. Using too much memory, for example, might result in the job being killed; using too many CPU cycles will slow down the job.</p> <p>Note\u02d0 The <code>srun</code> commands shown above work only to monitor a job submitted with sbatch. To monitor an interactive job, create multiple panes with <code>tmux</code> and start each process in its own pane.</p>"},{"location":"pages/high_perf_computing/running_jobs/#cancelling-jobs","title":"Cancelling jobs","text":"<p>Use scancel with the job ID to cancel a job:</p> <pre><code>$ scancel &lt;jobid&gt;\n</code></pre> <p>You can also use it to cancel all your jobs, or all your pending jobs:</p> <pre><code>$ scancel -u $USER\n$ scancel -t PENDING -u $USER\n</code></pre>"},{"location":"pages/high_perf_computing/standard_software_environment/","title":"Standard software environment","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Standard_software_environments */</p>"},{"location":"pages/high_perf_computing/standard_software_environment/#what-are-standard-software-environments","title":"What are standard software environments?","text":"<p>Our software environments are provided through a set of modules which allow you to switch between different versions of software packages. These modules are organized in a tree structure with the trunk made up of typical utilities provided by any Linux environment. Branches are compiler versions and sub-branches are versions of MPI or CUDA.</p> <p>Standard environments identify combinations of specific compiler and MPI modules that are used most commonly by our team to build other software. These combinations are grouped in modules named <code>StdEnv</code>.</p> <p>As of February 2023, there are four such standard environments, versioned 2023, 2020, 2018.3 and 2016.4, with each new version incorporating major improvements. Only versions 2020 and 2023 are actively supported.</p> <p>This page describes these changes and explains why you should upgrade to a more recent version.</p> <p>In general, new versions of software packages will get installed with the newest software environment.</p>"},{"location":"pages/high_perf_computing/standard_software_environment/#stdenv2023","title":"<code>**StdEnv/2023**</code>","text":"<p>This is the most recent iteration of our software environment. It uses GCC 12.3.0, Intel 2023.1, and Open MPI 4.1.5 as defaults.</p> <p>To activate this environment, use the command</p> <pre><code>[name@server ~]$ module load StdEnv/2023\n</code></pre> <p>Performance improvements</p> <p>The minimum CPU instruction set supported by this environment is AVX2, or more generally, x86-64-v3. Even the compatibility layer which provides basic Linux commands is compiled with optimisations for this instruction set.</p> <p>Changes of default modules</p> <p>GCC becomes the default compiler, instead of Intel. We compile with Intel only software which have been known to offer better performance using Intel. CUDA becomes an add-on to OpenMPI, rather than the other way around, i.e. CUDA-aware MPI is loaded at run time if CUDA is loaded. This allows to share a lot of MPI libraries across CUDA and non-CUDA branches.</p> <p>The following core modules have seen their default version upgraded:</p> <ul> <li>GCC 9.3 =&gt; GCC 12.3</li> <li>OpenMPI 4.0.3 =&gt; OpenMPI 4.1.5</li> <li>Intel compilers 2020 =&gt; 2023</li> <li>Intel MKL 2020 =&gt; Flexiblas 3.3.1 (with MKL 2023 or BLIS 0.9.0)</li> <li>CUDA 11 =&gt; CUDA 12</li> </ul>"},{"location":"pages/high_perf_computing/standard_software_environment/#stdenv2020","title":"<code>**StdEnv/2020**</code>","text":"<p>This is the most recent iteration of our software environment with the most changes so far. It uses GCC 9.3.0, Intel 2020.1, and Open MPI 4.0.3 as defaults.</p> <p>To activate this environment, use the command</p> <pre><code>[name@server ~]$ module load StdEnv/2020\n</code></pre> <p>Performance improvements</p> <p>Binaries compiled with the Intel compiler now automatically support both AVX2 and AVX512 instruction sets. In technical terms, we call them multi-architecture binaries, also known as fat binaries. This means that when running on a cluster such as Cedar and Graham which has multiple generations of processors, you don't have to manually load one of the arch modules if you use software packages generated by the Intel compiler.</p> <p>Many software packages which were previously installed either with GCC or with Intel are now installed at a lower level of the software hierarchy, which makes the same module visible, irrespective of which compiler is loaded. For example, this is the case for many bioinformatics software packages as well as the R modules, which previously required loading the gcc module. This could be done because we introduced optimizations specific to CPU architectures at a level of the software hierarchy lower than the compiler level.</p> <p>We also installed a more recent version of the GNU C Library, which introduces optimizations in some mathematical functions. This has increased the requirement on the version of the Linux Kernel (see below).</p> <p>Change in the compatibility layer</p> <p>Another enhancement for the 2020 release was a change in tools for our compatibility layer. The compatibility layer is between the operating system and all other software packages. This layer is designed to ensure that compilers and scientific applications will work whether they run on CentOS, Ubuntu, or Fedora. For the 2016.4 and 2018.3 versions, we used the Nix package manager, while for the 2020 version, we used Gentoo Prefix.</p> <p>Change in kernel requirement</p> <p>Versions 2016.4 and 2018.3 required a Linux kernel version 2.6.32 or more recent. This supported CentOS versions starting at CentOS 6. With the 2020 version, we require a Linux kernel 3.10 or better. This means it no longer supports CentOS 6, but requires CentOS 7 instead. Other distributions usually have kernels which are much more recent, so you probably don't need to change your distribution if you are using this standard environment on something other than CentOS.</p> <p>Module extensions</p> <p>With the 2020 environment, we started installing more Python extensions inside of their corresponding core modules. For example, we installed PyQt5 inside of the qt/5.12.8 module so that it supports multiple versions of Python. The module system has also been adjusted so you can find such extensions. For example, if you run</p> <pre><code>[name@server ~]$ module spider pyqt5\n</code></pre> <p>it will tell you that you can get this by loading the qt/5.12.8 module.</p>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/","title":"Using GPUs with Slurm","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Using_GPUs_with_Slurm */</p>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#introduction","title":"Introduction","text":"<p>To request one or more GPUs for a Slurm job, use this form:</p> <pre><code> --gpus-per-node=[type:]number\n</code></pre> <p>The square-bracket notation means that you must specify the number of GPUs, and you may optionally specify the GPU type. Valid types are listed in the Available GPUs table below, in the column headed \"Slurm type specifier\". Here are two examples:</p> <pre><code> --gpus-per-node=2\n --gpus-per-node=v100:1\n</code></pre> <p>The first line requests two GPUs per node, of any type available on the cluster. The second line requests one GPU per node, with the GPU being of the V100 type.</p> <p>The following form can also be used:</p> <pre><code> --gres=gpu[[:type]:number]\n</code></pre> <p>This is older, and we expect it will no longer be supported in some future release of Slurm. We recommend that you replace it in your scripts with the above <code>--gpus-per-node</code> form.</p> <p>There are a variety of other directives that you can use to request GPU resources: <code>--gpus</code>, <code>--gpus-per-socket</code>, <code>--gpus-per-task</code>, <code>--mem-per-gpu</code>, and <code>--ntasks-per-gp</code>u. Please see the Slurm documentation for sbatch for more about these. Our staff did not test all the combinations; if you don't get the result you expect, contact technical support.</p> <p>For general advice on job scheduling, see Running jobs.</p>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#multi-instance-gpu-mig-on-sdre","title":"Multi-Instance GPU (MIG) on SDRE","text":"<p>** To be developed**</p>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#examples","title":"Examples","text":""},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#single-core-job","title":"Single-core job","text":"<p>If you need only a single CPU core and one GPU:</p> <p>\ud83d\udce5click to download</p> gpu_serial_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\n./program                         # you can use 'nvidia-smi' for a test\n</code></pre>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#multi-threaded-job","title":"Multi-threaded job","text":"<p>For a GPU job which needs multiple CPUs in a single node:</p> <p>\ud83d\udce5click to download</p> gpu_threaded_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1         # Number of GPU(s) per node\n#SBATCH --cpus-per-task=6         # CPU cores/threads\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./program\n</code></pre> <p>For each GPU requested, we recommend no more than 10 CPU cores per GPU.</p>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#mpi-job","title":"MPI job","text":"<p>\ud83d\udce5click to download</p> gpu_mpi_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus=8                  # total number of GPUs\n#SBATCH --ntasks-per-gpu=1        # total of 8 MPI processes\n#SBATCH --cpus-per-task=6         # CPU cores per MPI process\n#SBATCH --mem-per-cpu=5G          # host memory per CPU core\n#SBATCH --time=0-03:00            # time (DD-HH:MM)\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsrun --cpus-per-task=$SLURM_CPUS_PER_TASK ./program\n</code></pre>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#whole-nodes","title":"Whole nodes","text":"<p>If your application can efficiently use an entire node and its associated GPUs, you will probably experience shorter wait times if you ask Slurm for a whole node. Use one of the following job scripts as a template.</p> <p>\ud83d\udce5click to download</p> gpu_node_job.sh<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --gpus-per-node=p100:4\n#SBATCH --ntasks-per-node=24\n#SBATCH --exclusive\n#SBATCH --mem=125G\n#SBATCH --time=3:00\n#SBATCH --account=def-someuser\nnvidia-smi\n</code></pre>"},{"location":"pages/high_perf_computing/using_gpus_with_slurm/#profiling-gpu-tasks","title":"Profiling GPU tasks","text":"<p>To be developed</p>"},{"location":"pages/high_perf_computing/using_modules/","title":"Using modules","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/Utiliser_des_modules/en*/</p> <p>Our servers can execute all software that runs under Linux. In the simplest case, the software you need will already be installed on one of the compute servers. It will then be accessible in the form of a \"module\". If this is not the case, you can either ask our staff to install it for you, or do it yourself.</p> <p>Modules are configuration files that contain instructions for modifying your software environment. This modular architecture allows multiple versions of the same application to be installed without conflict. For new servers, modules are managed with the Lmod tool developed at TACC. This tool replaces Environment Modules, which is used on most legacy servers. If you are familiar with this system you should not be too disoriented since \"Lmod\" was designed to be very similar to \"Environment Modules\". Refer to the #Lmod vs Environment Modules section for the main differences between the two systems.</p> <p>A \"modulefile\" contains the information needed to make an application or library available in the user's login session. Typically a module file contains instructions that modify or initialize environment variables such as <code>PATH</code> and <code>LD_LIBRARY_PATH</code> in order to use different installed programs. Note that the simple fact of loading a module doesn't execute the software in question. To learn the name of the program binary or the syntax for its use, you should read the documentation for this software. By using the <code>module</code> command, you shouldn't normally need to know the exact location or path of the software or library but you can nevertheless see such details about the module by means of the command <code>module show &lt;module-name&gt;</code>.</p>"},{"location":"pages/high_perf_computing/using_modules/#important-module-commands","title":"Important module commands","text":"<p>The command <code>module</code> has several subcommands. The normal syntax is</p> <pre><code>[name@server ~]$  module command [other options]\n</code></pre> <p>To see a list of available sub-commands, use</p> <pre><code>[name@server ~]$ module help\n</code></pre>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-spider","title":"Sub-command <code>spider</code>","text":"<p>The <code>spider</code> sub-command searches the complete tree of all modules in the current standard software environment and displays it.</p> <pre><code>[name@server ~]$ module spider\n</code></pre> <p>If you specify the name of an application, for example with</p> <pre><code>[name@server ~]$ module spider openmpi\n</code></pre> <p>this will show you a list of all available versions of the application.</p> <p>If you specify the name of the application along with a version number, for example with</p> <pre><code>[name@server ~]$ module spider openmpi/4.0.3\n</code></pre> <p>this will display a list of the modules you must load in order to access this version.</p>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-avail","title":"Sub-command <code>avail</code>","text":"<p>To list the modules you can load, use</p> <pre><code>[name@server ~]$ module avail\n</code></pre> <p>You can obtain a list of modules available for a particular library or tool, for example modules related to openmpi:</p> <pre><code>[name@server ~]$ module avail openmpi\n</code></pre> <p>Note that the <code>module avail</code> command may not list some modules that are incompatible with the modules you have loaded. To see the list of all modules other than those you've loaded and which are available for you, use the <code>spider</code> sub-command documented above.</p>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-list","title":"Sub-command <code>list</code>","text":"<p>The sub-command <code>list</code> lists the modules that are currently loaded in your environment.</p> <pre><code>[name@server ~]$ module list\n</code></pre>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-load","title":"Sub-command <code>load</code>","text":"<p>The sub-command <code>load</code> lets you load a given module. For example,</p> <pre><code>[name@server ~]$ module load gcc/9.3\n</code></pre> <p>allows you to access the GCC compiler suite, version 9.3.</p> <p>You can load more than one module with a single command. For example,</p> <pre><code>[name@server ~]$ module load gcc/9.3 openmpi/4.0\n</code></pre> <p>will load at the same time the GCC 9.3 compilers and the Open MPI library 4.0, compiled for GCC.</p> <p>If you load a module that is incompatible with one you already have loaded, Lmod will tell you that it has replaced the old module with a new one. This can occur especially for compilers and MPI implementations.</p>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-unload","title":"Sub-command <code>unload</code>","text":"<p>In contrast with the <code>load</code> sub-command, <code>unload</code> removes a module from your environment. For example,</p> <p><pre><code>[name@server ~]$ module unload gcc/9.3\n</code></pre> would remove the GCC 9.3 compiler suite from your environment.</p> <p>If you have other modules loaded that depend on this compiler, Lmod will tell you that they have been disabled.</p>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-purge","title":"Sub-command purge","text":"<p>The sub-command <code>purge</code> allows you to remove all the modules you have loaded with a single command.</p> <pre><code>[nom@serveur ~]$ module purge\n</code></pre> <p>Some modules may be marked \"sticky\" (permanent) by system administrators. These will not be unloaded.</p>"},{"location":"pages/high_perf_computing/using_modules/#sub-commands-show-help-and-whatis","title":"Sub-commands <code>show</code>, <code>help</code> and <code>whatis</code>","text":"<p>The sub-commands <code>show</code>, <code>help</code> and <code>whatis</code> provide additional information about a given module. The <code>show</code> sub-command displays the entire module, <code>help</code> displays a help message, and whatis shows a description of the module.</p> <pre><code>[nom@serveur ~]$ module help gcc/9.3\n</code></pre>"},{"location":"pages/high_perf_computing/using_modules/#sub-command-apropos-or-keyword","title":"Sub-command <code>apropos</code> or <code>keyword</code>","text":"<p>The sub-commands <code>apropos</code> or <code>keyword</code> allow you to search for a keyword in all modules. If you don't know which module is appropriate for your calculation, you can search by description.</p>"},{"location":"pages/high_perf_computing/using_modules/#loading-modules-automatically","title":"Loading modules automatically","text":"<p>We advise against loading modules automatically in your .bashrc. Instead we recommend that you load modules only when required, for example in your job scripts. To facilitate the repeated loading of a large number of modules we recommend you use a module collection.</p>"},{"location":"pages/high_perf_computing/using_modules/#module-collections","title":"Module collections","text":"<p>Lmod allows you to create a collection of modules. To do so, first load the desired modules. For example:</p> <pre><code>[name@server ~]$ module load gcc/9.3 openmpi/4.0.3 mkl\n</code></pre> <p>Then use the <code>save</code> sub-command to save this collection:</p> <pre><code>[name@server ~]$ module save my_modules\n</code></pre> <p>The <code>my_modules</code> argument is a name you give to the collection.</p> <p>Then in a later session or in a job you can restore the collection with the command</p> <pre><code>[name@server ~]$ module restore my_modules\n</code></pre>"},{"location":"pages/high_perf_computing/using_modules/#hidden-modules","title":"Hidden modules","text":"<p>Some modules are hidden. You may ignore them. They are typically modules that you don't have to load manually. They are loaded automatically based on other modules.</p>"},{"location":"pages/high_perf_computing/using_modules/#module-hierarchy","title":"Module hierarchy","text":"<p>Many HPC clusters around the world use a flat module structure: All modules are at the same level. This becomes problematic when many combinations of versions of different modules are available on a system. For example, if you need to use the FFTW library and the module <code>fftw</code> is available in several versions, including a version compiled with GCC 9.3 and Open MPI 4.0, you might see modules named openmpi/4.0_gcc9.3 and fftw/3.8_gcc9.3_openmpi4.0. This is neither elegant nor practical. To solve this problem we use a hierarchy of modules. Rather than using the command</p> <pre><code>[name@server ~]$ module load gcc/9.3 openmpi/4.0_gcc9.3 fftw/3.8_gcc9.3_openmpi4.0\n</code></pre> <p>you instead use</p> <pre><code>[name@server ~]$ module load gcc/9.3 openmpi/4.0 fftw/3.8\n</code></pre> <p>This is made possible by using a module hierarchy. The fftw/3.8 module that is loaded will not be the same one that would be loaded if you had previously loaded the Intel compilers instead of GCC.</p> <p>The inconvenience of using a module hierarchy is that, since modules can have the same name, only the modules that are compatible with the \"parent\" modules are displayed by the <code>module avail</code> command. Loading a parent module is therefore a prerequisite to loading some modules. To get complete information, the module system provides the <code>module spider</code> command. It scans the entire hierarchy and displays all the modules. By specifying a module and a particular version, it is then possible to see which paths in the hierarchy enable the desired module to be loaded.</p>"},{"location":"pages/high_perf_computing/what_is_a_scheduler/","title":"What is a scheduler?","text":"<p>/* Reference: https://docs.alliancecan.ca/wiki/What_is_a_scheduler%3F */</p>"},{"location":"pages/high_perf_computing/what_is_a_scheduler/#what-is-a-job","title":"What is a job?","text":"<p>On computers, we are most often familiar with graphical user interfaces (GUIs). There are windows, menus, buttons; we click here and there and the system responds. On our servers, the environment is different. To begin with, you control it by typing, not clicking. This is called a command line interface. Furthermore, a program you would like to run may not begin immediately, but may instead be put on a waiting list. When the necessary CPU cores are available it will begin, otherwise jobs would interfere with each other leading to performance loss.</p> <p>You prepare a small text file called a job script that basically says what program to run, where to get the input, and where to put the output. You submit this job script to a piece of software called the scheduler which decides when and where it will run. Once the job has finished, you can retrieve the results of the calculation. Normally there is no interaction between you and the program while the job is running, although you can check on its progress if you wish.</p> <p>Here's a very simple job script:</p> <p>``` sh title=\"simple_job.sh\" click to download</p>"},{"location":"pages/high_perf_computing/what_is_a_scheduler/#binbash","title":"!/bin/bash","text":""},{"location":"pages/high_perf_computing/what_is_a_scheduler/#sbatch-time000100","title":"SBATCH --time=00:01:00","text":"<p>echo 'Hello, world!' sleep 30 <pre><code>It runs the programs `echo` and `sleep`, there is no input, and the output will go to a default location. Lines starting with #SBATCH are directives to the scheduler, providing information about what the job needs to run. This job, for example, only needs one minute of run time (00:01:00).\n\n## **The job scheduler**\n\n----\n\nThe job scheduler is a piece of software with multiple responsibilities. It must\n\n- maintain a database of jobs,\n- enforce policies regarding limits and priorities,\n- ensure resources are not overloaded, for example by only assigning each CPU core to one job at a time,\n- decide which jobs to run and on which compute nodes,\n- launch them on those nodes, and\n- clean up after each job finishes.\n\nOn our environment, these responsibilities are handled by the [Slurm Workload Manager](https://en.wikipedia.org/wiki/Slurm_Workload_Manager). All the examples and syntax shown on this page are for Slurm.\n\n## **Requesting resources**\n\n----\n\nYou use the job script to ask for the resources needed to run your calculation. Among the resources associated with a job are time and number of processors. In the example above, the time requested is one minute and there will be one processor allocated by default since no specific number is given. Please refer to [Examples of job scripts](/pages/high_perf_computing/running_jobs/#examples-of-job-scripts) for other types of requests such as multiple processors, memory capacity and special processors such as [GPUs](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units).\n\nIt is important to specify those parameters well. If you ask for less than the calculation needs, the job will be killed for exceeding the requested time or memory limit. If you ask for more than it needs, the job may wait longer than necessary before it starts, and once running it will needlessly prevent others from using those resources.\n\n## **A basic Slurm job**\n\n----\n\nWe can submit the job script `simple_job.sh` shown above with [sbatch](https://slurm.schedmd.com/sbatch.html):\n</code></pre> [someuser@host ~]$ sbatch simple_job.sh Submitted batch job 1234 [someuser@host ~]$ sq    JOBID     USER      ACCOUNT      NAME  ST  TIME_LEFT NODES CPUS    GRES MIN_MEM NODELIST (REASON)     1234 someuser def-someprof  simple_j   R       0:33     1    1  (null)    256M blg9876 (None) [someuser@host ~]$ cat slurm-1234.out Hello, world! <pre><code>Look at the ST column in the output of sq to determine the status of your jobs. The two most common states are PD for *pending* and R for *running*. When the job has finished, it no longer appears in the `sq` output.\n\nNotice that each job is assigned a *job ID*, a unique identification number printed when you submit the job --- 1234 in this example. You can have more than one job in the system at a time, and the ID number can be used to distinguish them even if they have the same name. And finally, because we didn't specify anywhere else to put it the output is placed in a file named with the same job ID number, `slurm\u20111234.out`.\n\nYou can also specify options to `sbatch` on the command line. So for example,\n</code></pre> [someuser@host ~]$ sbatch --time=00:30:00 simple_job.sh  <pre><code>will change the time limit of the job to 30 minutes. Any option can be overridden in this way.\n\n## **Choosing where the output goes**\n\n----\n\nIf you want the output file to have a more distinctive name than `slurm\u20111234.out`, you can use `--output` to change it. The following script sets a *job name* which will appear in the `squeue` output, and sends the output to a file prefixed with the job name and containing the job ID number, for exemple `test-1234.out`.\n\n``` sh title=\"name_output.sh\" [click to download](files/high_perf_computing/what_is_a_scheduler/name_output.sh){:download=\"name_output.sh\"}\n#!/bin/bash\n#SBATCH --time=00:01:00\n#SBATCH --job-name=test\n#SBATCH --output=test-%J.out\necho 'Hello, world!'\n</code></pre></p> <p>Error output will normally appear in the same file, just as it would if you were typing commands interactively. If you wish you can split the standard error channel (stderr) from the standard output channel (stdout) by specifying a file name with the <code>\u2011e</code> option.</p>"},{"location":"pages/high_perf_computing/what_is_a_scheduler/#accounts-and-projects","title":"Accounts and projects","text":"<p>Information about your job, like how long it waited, how long it ran, and how many cores it used, is recorded so we can monitor our quality of service and so we can report to our funders how their money is spent. Every job must have an associated account name corresponding to a resource allocation project.</p> <pre><code>#SBATCH --account=def-user-ab\n</code></pre> <p>If you try to submit a job with <code>sbatch</code> without supplying an account name, and one is needed, you will be shown a list of valid account names to choose from.</p>"},{"location":"pages/software_catalog/apptainer/","title":"Apptainer","text":"<p>This section is under construction...</p>"},{"location":"pages/software_catalog/available_python_wheels/","title":"Available Python wheels","text":"<p>This section is under construction...</p>"},{"location":"pages/software_catalog/available_software/","title":"Available software","text":"<p>This section is under construction...</p>"},{"location":"pages/software_catalog/easybuild/","title":"Easybuild","text":"<p>This section is under construction...</p>"},{"location":"pages/software_catalog/virtual_environment/","title":"Creating and using virtual environment","text":"<p>With each version of Python, we provide the tool virtualenv. This tool allows users to create virtual environments within which you can easily install Python packages. These environments allow one to install many versions of the same package, for example, or to compartmentalize a Python installation according to the needs of a specific project. Usually you should create your Python virtual environment(s) in your /home directory or in one of your /project directories. (See \"Creating virtual environments inside of your jobs\" below for a third alternative.)</p> <p>To create a virtual environment, make sure you have selected a Python version with module load python/X.Y.Z as shown above in section Loading a Python module. If you expect to use any of the packages listed in section SciPy stack above, also run module load scipy-stack/X.Y.Z. Then enter the following command, where ENV is the name of the directory for your new environment:</p> <pre><code>[name@server ~]$ virtualenv --no-download ENV\n</code></pre> <p>Once the virtual environment has been created, it must be activated:</p> <pre><code>[name@server ~]$ source ENV/bin/activate\n</code></pre> <p>You should also upgrade pip in the environment:</p> <pre><code>[name@server ~]$ pip install --no-index --upgrade pip\n</code></pre> <p>To exit the virtual environment, simply enter the command <code>deactivate</code>:</p> <pre><code>(ENV) [name@server ~] deactivate\n</code></pre> <p>You can now use the same virtual environment over and over again. Each time:</p> <ul> <li>Load the same environment modules that you loaded when you created the virtual environment, e.g. module load python scipy-stack</li> <li>Activate the environment, source ENV/bin/activate</li> </ul>"},{"location":"pages/software_catalog/virtual_environment/#installing-packages","title":"Installing packages","text":"<p>Once you have a virtual environment loaded, you will be able to run the pip command. This command takes care of compiling and installing most of Python packages and their dependencies. A comprehensive index of Python packages can be found at PyPI.</p> <p>All of <code>pip</code>'s commands are explained in detail in the user guide. We will cover only the most important commands and use the Numpy package as an example.</p> <p>We first load the Python interpreter:</p> <pre><code>[name@server ~]$ module load python/3.10\n</code></pre> <p>We then activate the virtual environment, previously created using the <code>virtualenv</code> command:</p> <pre><code>[name@server ~]$ source ENV/bin/activate\n</code></pre> <p>Finally, we install the latest stable version of Numpy:</p> <pre><code>(ENV) [name@server ~] pip install numpy --no-index\n</code></pre> <p>The pip command can install packages from a variety of sources, including PyPI and prebuilt distribution packages called Python wheels. We provide Python wheels for a number of packages. In the above example, the <code>--no-index</code> option tells <code>pip</code> to not install from PyPI, but instead to install only from locally available packages, i.e. our wheels.</p> <p>Whenever we provide a wheel for a given package, we strongly recommend to use it by way of the <code>--no-index</code> option. Compared to using packages from PyPI, wheels that have been compiled by our staff can prevent issues with missing or conflicting dependencies, and were optimized for our clusters hardware and libraries. See Available wheels.</p> <p>If you omit the <code>--no-inde</code>x option, pip will search both PyPI and local packages, and use the latest version available. If PyPI has a newer version, it will be installed instead of our wheel, possibly causing issues. If you are certain that you prefer to download a package from PyPI rather than use a wheel, you can use the <code>--no-binary</code> option, which tells <code>pip</code> to ignore prebuilt packages entirely. Note that this will also ignore wheels that are distributed through PyPI, and will always compile the package from source.</p> <p>To see where the <code>pip</code> command is installing a python package from, diagnosing installation issues, you can tell it to be more verbose with the <code>-vvv</code> option. It is also worth mentioning that when installing multiple packages it is advisable to install them with one command as it helps pip resolve dependencies.</p>"},{"location":"pages/software_catalog/virtual_environment/#creating-virtual-environments-inside-of-your-jobs","title":"Creating virtual environments inside of your jobs","text":"<p>Parallel filesystems such as the ones used on our clusters are very good at reading or writing large chunks of data, but can be bad for intensive use of small files. Launching a software and loading libraries, such as starting Python and loading a virtual environment, can be slow for this reason.</p> <p>As a workaround for this kind of slowdown, and especially for single-node Python jobs, you can create your virtual environment inside of your job, using the compute node's local disk. It may seem counter-intuitive to recreate your environment for every job, but it can be faster than running from the parallel filesystem, and will give you some protection against some filesystem performance issues. This approach, of creating a node-local virtualenv, has to be done for each node in the job, since the virtualenv is only accessible on one node. Following job submission script demonstrates how to do this for a single-node job:</p> <p>This section is under construction...</p>"},{"location":"pages/storage_data/data_transfer/","title":"Data transfer","text":""},{"location":"pages/storage_data/data_transfer/#to-and-from-your-personal-computer","title":"To and from your personal computer","text":"<p>You will need software that supports secure transfer of files between your computer and our machines. The commands scp and sftp can be used in a command-line environment on Linux or Mac OS X computers. On Microsoft Windows platforms, MobaXterm offers both a graphical file transfer function and a command-line interface via SSH, while WinSCP is another free program that supports file transfer. Setting up a connection to a machine using SSH keys with WinSCP can be done by following the steps in this link. PuTTY comes with pscp and psftp which are essentially the same as the Linux and Mac command line programs.</p>"},{"location":"pages/storage_data/data_transfer/#sftp","title":"SFTP","text":"<p>SFTP (Secure File Transfer Protocol) uses the SSH protocol to transfer files between machines which encrypts data being transferred.</p> <p>For example, you can connect to a remote machine at ADDRESS as user USERNAME with SFTP to transfer files like so:</p> <pre><code>[name@server]$ sftp USERNAME@ADDRESS\nThe authenticity of host 'ADDRESS (###.###.###.##)' can't be established.\nRSA key fingerprint is ##:##:##:##:##:##:##:##:##:##:##:##:##:##:##:##.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'ADDRESS,###.###.###.##' (RSA) to the list of known hosts.\nUSERNAME@ADDRESS's password:\nConnected to ADDRESS.\nsftp&gt;\n</code></pre> <p>or using an SSH Key for authentication using the -i option</p> <pre><code>[name@server]$ sftp -i /home/name/.ssh/id_rsa USERNAME@ADDRESS\nConnected to ADDRESS.\nsftp&gt;\n</code></pre> <p>which returns the <code>sftp&gt;</code> prompt where commands to transfer files can be issued. To get a list of commands available to use at the sftp prompt enter the <code>help</code> command.</p> <p>There are also a number of graphical programs available for Windows, Linux and Mac OS, such as WinSCP and MobaXterm (Windows), filezilla (Windows, Mac, and Linux), and cyberduck (Mac and Windows).</p>"},{"location":"pages/storage_data/data_transfer/#scp","title":"SCP","text":"<p>SCP stands for Secure Copy Protocol. Like SFTP it uses the SSH protocol to encrypt data being transferred. It does not support synchronization like rsync. Some examples of the most common use of SCP include</p> <pre><code>[name@server ~]$ scp example.txt username@sdre.ualberta.ca:targetfolder/\n</code></pre> <p>which will copy the file example.txt from the current directory on my local computer to the directory $HOME/targetfolder on the SDRE. To copy a file, output.dat from my project space on the SDRE to my local computer I can use a command like</p> <pre><code>[name@server ~]$ scp username@sdre.ualberta.ca:projects/def-jdoe/username/results/output.dat .\n</code></pre> <p>Many other examples of the use of SCP are shown here. Note that you always execute this <code>scp</code> command on your local computer, not the remote cluster - the SCP connection, regardless of whether you are transferring data to or from the remote cluster, should always be initiated from your local computer.</p> <p>SCP supports the option <code>-r</code> to recursively transfer a set of directories and files. We recommend against using <code>scp -r</code> to transfer data into <code>/project</code> because the setgid bit is turned off in the created directories, which may lead to <code>Disk quota exceeded</code> or similar errors if files are later created there (see Disk quota exceeded error on /project filesystems).</p> <p>Note if you chose a custom SSH key name, i.e. something other than the default names: <code>id_dsa</code>, <code>id_ecdsa</code>, <code>id_ed25519</code> and <code>id_rsa</code>, you will need to use the <code>-i</code> option of scp and specify the path to your private key before the file paths via</p> <pre><code>[name@server ~]$ scp -i /path/to/key example.txt username@sdre.ualberta.ca:targetfolder/\n</code></pre>"},{"location":"pages/storage_data/data_transfer/#transfer-from-rdss-to-sdre","title":"Transfer from RDSS to SDRE","text":"<p>This section is under construction...</p>"},{"location":"pages/storage_data/storage_overview/","title":"Storage Overview","text":"<p>This section is under construction...</p>"},{"location":"pages/training/connecting_to_hpc_video/","title":"Connecting to HPC","text":""},{"location":"pages/training/connecting_to_hpc_video/#unverifited-device","title":"Unverifited Device","text":""},{"location":"pages/training/connecting_to_hpc_video/#step-1-connecting-to-uofaguest-vpn","title":"Step 1: Connecting to UofA/guest VPN","text":""},{"location":"pages/training/connecting_to_hpc_video/#step-2-launching-vcs","title":"Step 2: Launching VCS","text":""},{"location":"pages/training/connecting_to_hpc_video/#step-3-connecting-via-ssh","title":"Step 3: Connecting via SSH","text":""},{"location":"pages/training/connecting_to_hpc_video/#verified-device","title":"Verified Device","text":""},{"location":"pages/training/connecting_to_hpc_video/#step-1-connecting-to-sdre-vpn","title":"Step 1: Connecting to SDRE VPN","text":""},{"location":"pages/training/connecting_to_hpc_video/#step-2-connecting-via-ssh","title":"Step 2: Connecting via SSH","text":""},{"location":"pages/training/rc_bootcamp/","title":"Research Computing Bootcamp","text":"<p>Research Computing Bootcamps are multi-day events, comprising several workshops designed for faculty members, students, postdoctoral fellows, and other researchers across campus to gain knowledge on how to use ARC in their research. Bootcamps also connect you with colleagues working in the same space to share knowledge, best practices, and get answers to your questions. These events run three times a year in fall, winter, and spring. Workshops cover a variety of topics such as Python, ARC clusters, and research cloud computing.</p>"},{"location":"pages/training/rc_bootcamp/#hpc","title":"HPC","text":""},{"location":"pages/training/rc_bootcamp/#essentials","title":"Essentials","text":""},{"location":"pages/training/rc_bootcamp/#shell-basics","title":"Shell Basics","text":""},{"location":"pages/training/rc_bootcamp/#parallelism","title":"Parallelism","text":""},{"location":"pages/training/rc_bootcamp/#scripting-basics","title":"Scripting Basics","text":""},{"location":"pages/training/rc_bootcamp/#scheduling","title":"Scheduling","text":"<p>Following topics and more are covered:</p> <ul> <li>What do you do when things go wrong, what happens when your job fails and how to fix it.</li> <li>How do you figure out what resources your job's needs on the cluster?</li> <li>Tips and tricks about being more productive using a cluster.</li> <li>Practice creating and submitting more complex jobs: parallel jobs, job arrays, memory requirements, job dependencies, gpus</li> <li>GPUs introduce complexity into scheduling, while a basic GPU job is simple, to be effective requires a much higher skill set.</li> </ul>"},{"location":"pages/training/rc_bootcamp/#python","title":"Python","text":""},{"location":"pages/training/rc_bootcamp/#introduction-to-python","title":"Introduction to Python","text":""},{"location":"pages/training/rc_bootcamp/#intermediate-python-introduction-to-machine-learning","title":"Intermediate Python: Introduction to Machine Learning","text":""},{"location":"pages/training/rc_bootcamp/#regular-expression","title":"Regular Expression","text":""},{"location":"pages/training/slurm_job_submission/","title":"Slurm Job Submission Video","text":""},{"location":"pages/training/slurm_job_submission/#part-1-batch-scripts-and-components","title":"Part 1: Batch scripts and components","text":""},{"location":"pages/training/slurm_job_submission/#part-2-commands","title":"Part 2: Commands","text":""},{"location":"pages/training/slurm_job_submission/#part-3-resource-availability","title":"Part 3: Resource availability","text":""}]}