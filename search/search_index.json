{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SDRE","text":"<p>For full documentation visit mkdocs.org.</p> <p>For about information about Nebulas tempore, visit About Page</p> <p>this is for testing color,  this should be colored,italic and bold text .</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"alternate/","title":"Alternate","text":"<p>Systems and services</p> <ul> <li>2025 infrastructure renewal</li> <li>General-purpose clusters<ul> <li>B\u00e9luga, Cedar, Graham</li> </ul> </li> <li>Services<ul> <li>Available software</li> <li>Cloud computing service</li> </ul> </li> </ul> <p>How-to guides</p> <ul> <li>Getting started<ul> <li>Niagara Quick Start Guide</li> <li>Cloud Quick Start Guide</li> </ul> </li> <li>Running jobs<ul> <li>Installing software yourself</li> <li>Programming guide</li> </ul> </li> </ul> <p>Research Resources</p> <ul> <li>Data Storage<ul> <li>Nextcloud, Globus Transfer</li> </ul> </li> <li>Research Software<ul> <li>MATLAB, TensorFlow</li> </ul> </li> </ul> <p>Support &amp; FAQs</p> <ul> <li>Need Help?<ul> <li>Technical Support</li> <li>FAQs</li> </ul> </li> </ul>"},{"location":"linuxintro/","title":"Linux Introduction","text":"<p>This article is aimed at Windows and Mac users who do not have or have very little experience in UNIX environments. It should give you the necessary basics to access the compute servers and being quickly able to use them. Connections to the servers use the SSH protocol, in text mode. You do not use a graphical interface (GUI) but a console. Note that Windows executables do not run on our servers without using an emulator.</p>"},{"location":"linuxintro/#obtaining-help","title":"Obtaining help","text":"<p>Generally, UNIX commands are documented in the reference manuals that are available on the servers. To access those from a terminal:</p> <pre><code>[name@server ~]$ man command\n</code></pre> <p><code>man</code> uses <code>less</code> (see the section Viewing and editing files), and you must press <code>q</code> to exit this program.</p> <p>By convention, the executables themselves contain some help on how to use them. Generally, you invoke this help using the command line argument <code>-h</code> or <code>--help</code>, or in certain cases, <code>-help</code>. For example,</p> <pre><code>[name@server ~]$ ls --help\n</code></pre>"},{"location":"linuxintro/#orienting-yourself-on-a-system","title":"Orienting yourself on a system","text":"<p>Following your connection, you are directed to your <code>$HOME</code> directory (the UNIX word for folder) for your user account. When your account is created, your <code>$HOME</code> only contains a few hidden configuration files that start with a period (.), and nothing else.</p> <p>On a Linux system, you are strongly discouraged to create files or directories that contain names with spaces or special characters, including accents.</p> Material for MkDocsInsiders <pre><code>pip install mkdocs-material\n</code></pre> <pre><code>pip install git+https://github.com/squidfunk/mkdocs-material-insiders.git\n</code></pre>"},{"location":"linuxintro/#nebulas-tempore-galeaque-negabo-palmae-nulla","title":"Nebulas tempore galeaque negabo palmae nulla","text":"<p>Vidit et posuere pater. Tunc me hanc, meos et iuvenis Cupido aequoreae me.</p> <pre><code>mpeg_core_contextual = vram.memory_metadata(boot_trackball_vista +\n        toslinkCamera * trinitron, processorWeb);\nvar memory = wordYahoo;\nvar app = ccdRoomServices(upload_leopard(key, volume_configuration_cad,\n        olap_computer_netbios), keySoftDaw(publishingNavigationDot,\n        bar_dvd_kvm, -3));\niscsiRomT = thermistorWidget;\n</code></pre>"},{"location":"linuxintro/#putat-hominis-quirini-procul-lucem","title":"Putat hominis Quirini procul lucem","text":"<p>Saepe quae in corpus eripuit animam instantiaque illa custodemque Indigetem pariter, non dempto est medicamine nomen, e. Membra residens ferarum et virgo illo habet, profundum leonis fortis ab comas liceat, et cadentum? Fatalis consternatique subdit ad gnato. Lenta longeque commemorat fuit.</p> <ol> <li>Devia saepe malum</li> <li>Nivea tantaeque pectore tamen luce nepotem roseum</li> <li>Nurusque et numen pectora nonne</li> </ol> <p>Foret iactasque animi. Parnasi et seditio utrumque letoque querenti edita moveant in Mycale. Editus ullis domos quas fronde, in auctore unus, ponitur sed iam, caelum. Est ipse magnorum, at hasta quod dextra oderat sanguine. Enim cremabis tergoque praedictaque passim tamen: Schoeneia inachus sonantibus silicem, lassos undis scit spirandi rigorem!</p>"},{"location":"quicklink/","title":"Quick Link","text":"Systems and services <ul> <li>2025 infrastructure renewal</li> <li>General-purpose clusters</li> <ul> <li>B\u00e9luga, Cedar, Graham</li> </ul> <li>Services</li> <ul> <li>Available software</li> <li>Cloud computing service</li> </ul> </ul> How-to guides <ul> <li>Getting started</li> <ul> <li>Niagara Quick Start Guide</li> <li>Cloud Quick Start Guide</li> </ul> <li>Running jobs</li> <ul> <li>Installing software yourself</li> <li>Programming guide</li> </ul> </ul> Research Resources <ul> <li>Nextcloud storage</li> <li>Research Software</li> </ul> Support &amp; FAQs <ul> <li>Technical Support</li> <li>FAQs</li> </ul>"},{"location":"slurmintro/","title":"Using GPUs with Slurm","text":""},{"location":"slurmintro/#single-core-job","title":"Single-core Job","text":"<p>If you need only a single CPU core and one GPU:</p> <p>\ud83d\udce5 Download gpu_serial_job.sh</p> gpu_serial_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\n./program                         # you can use 'nvidia-smi' for a test\n</code></pre>"},{"location":"slurmintro/#multi-threaded-job","title":"Multi-threaded Job","text":"<p>For a GPU job which needs multiple CPUs in a single node:</p> <p>\ud83d\udce5 Download gpu_threaded_job.sh</p> gpu_threaded_job.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1         # Number of GPU(s) per node\n#SBATCH --cpus-per-task=6         # CPU cores/threads\n#SBATCH --mem=4000M               # memory per node\n#SBATCH --time=0-03:00\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./program\n</code></pre> <p>For each GPU requested, we recommend</p> <ul> <li>on B\u00e9luga, no more than 10 CPU cores;</li> <li>on Cedar,</li> <li>no more than 6 CPU cores per P100 GPU (p100 and p100l);</li> <li>no more than 8 CPU cores per V100 GPU (v100l);</li> <li>on Graham, no more than 16 CPU cores.</li> </ul>"},{"location":"storagefile/","title":"Storage and File Management","text":""},{"location":"storagefile/#storage-types","title":"Storage Types","text":"<p>Unlike your personal computer, our systems will typically have several storage spaces or filesystems and you should ensure that you are using the right space for the right task. In this section we will discuss the principal filesystems available on most of our systems and the intended use of each one along with some of its characteristics.</p> <ul> <li> <p>HOME: While your home directory may seem like the logical place to store all your files and do all your work, in general this isn't the case; your home normally has a relatively small quota and doesn't have especially good performance for writing and reading large amounts of data. The most logical use of your home directory is typically source code, small parameter files and job submission scripts.</p> </li> <li> <p>PROJECT: The project space has a significantly larger quota and is well adapted to sharing data among members of a research group since it, unlike the home or scratch, is linked to a professor's account rather than an individual user. The data stored in the project space should be fairly static, that is to say the data are not likely to be changed many times in a month. Otherwise, frequently changing data, including just moving and renaming directories, in project can become a heavy burden on the tape-based backup system.</p> </li> <li> <p>SCRATCH: For intensive read/write operations on large files (&gt; 100 MB per file), scratch is the best choice. However, remember that important files must be copied off scratch since they are not backed up there, and older files are subject to purging. The scratch storage should therefore be used for temporary files: checkpoint files, output from jobs and other data that can easily be recreated. Do not regard SCRATCH as your normal storage! It is for transient files that you can afford to lose.</p> </li> <li> <p>SLURM_TMPDIR: While a job is running, the environment variable $SLURM_TMPDIR holds a unique path to a temporary folder on a fast, local filesystem on each compute node allocated to the job. When the job ends, the directory and its contents are deleted, so $SLURM_TMPDIR should be used for temporary files that are only needed for the duration of the job. Its advantage, compared to the other networked filesystem types above, is increased performance due to the filesystem being local to the compute node. It is especially well-suited for large collections of small files (for example, smaller than a few megabytes per file). Note that this filesystem is shared between all jobs running on the node, and that the available space depends on the compute node type. A more detailed discussion of using $SLURM_TMPDIR is available at this page.</p> </li> </ul>"},{"location":"storagefile/#filesystem-quotas-and-policies","title":"Filesystem Quotas and Policies","text":"CedarGrahamB\u00e9luga and NarvalNiagara Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes Filesystem Default Quota Lustre-based Backed up Purged Available by Default Mounted on compute nodes Home space 50 GB and 500K files per user Yes Yes No Yes Yes Scratch Space 20 TB and 1M files per user Yes No Files older than 60 days are purged Yes Yes Project Space 1 TB and 500K files per group Yes Yes No Yes Yes"}]}